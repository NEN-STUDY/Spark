{"cells":[{"cell_type":"markdown","source":["# Dataset\n- Dataset은 구조적 API의 기본 데이터 타입\n  - DataFrame은 Row 타입의 Dataset\n\n- Dataset은 자바 가상 머신을 사용하는 언어인 <strong>스칼라와 자바</strong>에선만 사용 가능\n\n- 도메인별 특정 객체를 효과적으로 지원하기 위해서는 <strong>인코더(encoder)</strong>의 개념이 필요\n  - 스칼라에서는 <strong>스키마가 정의된 케이스 클래스 객체</strong>를 사용해 Dataset 정의\n  - 자바에서는 <strong>자바빈객체</strong>를 사용해 Dataset 정의\n\n- 인코더: 도메인별 특정 객체를 스파크의 내부 데이터 타입으로 매핑하는 시스템\n  - Dataset API 사용 시 스파크는 데이터셋에 접근할 때마다 <strong>사용자 정의 데이터 타입</strong>으로 변환\n    - 이 변환 작업은 느림\n    - 하지만 더 많은 유연성 제공"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee4802e5-38a7-47dd-9ea9-a4d46364dbef"}}},{"cell_type":"markdown","source":["## Dataset을 사용할 시기\n- Dataset을 사용해야 하는 두 가지 이유\n  - DataFrame 기능만으로는 수행할 연산을 표현할 수 없는 경우\n  - 성능 저하를 감수하더라도 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶은 경우\n\n- 단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 트랜스포메이션을 사용하려면 Dataset이 적합\n  - 케이스 클래스로 구현된 데이터 타입을 사용해 모든 데이터와 트랜스포메이션을 정의하면 재사용 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aee5d5f6-763e-46fc-9b45-4a4bace65a9d"}}},{"cell_type":"markdown","source":["## Dataset 생성\n- Dataset을 생성하는 것은 수동 작업이므로 정의할 스키마를 미리 알아야함"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0118e114-b55b-45e0-9664-4c89179903db"}}},{"cell_type":"markdown","source":["### 자바: Encoders\n- 데이터 타입 클래스를 정의하고 DataFrame에 지정해 인코딩"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fc2f990-7b70-4569-b361-332f4e37817a"}}},{"cell_type":"code","source":["import org.apache.spark.sql.Encoders;\n\npublic class Flight implements Serializable{\n  String DEST_COUNTRY_NAME;\n  String ORIGIN_COUNTRY_NAME;\n  Long DEST_COUNTRY_NAME;\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e2c0f9-43c4-4adb-971d-c60935eeb402"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["Dataset<Flight> flights = spark.read.parquet('/FileStore/tables/2010_summary.parquet/').as(Encoders.bean(Flight.class))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04bbb6f6-ab1b-4a0c-a4ad-780eca6e74f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 스칼라: 케이스 클래스\n- 스칼라에서 Dataset을 생성하려면 스칼라 case class 구문을 사용해 데이터 타입을 정의해야함\n\n- 케이스 클래스는 다음과 같은 특징을 가진 정규 클래스임\n  - 불변성\n    - 객체들이 언제 어디서 변경되었는지 추적할 필요 없음\n    \n  - 패턴 매칭으로 분해 가능\n    - 로직 분기를 단순화해 버그를 줄이고 가독성을 좋게 만듦\n    \n  - 참조값 대신 클래스 구조를 기반으로 비교\n    - 값으로 비교하면 인스턴스를 마치 원시 데이터 타입의 값처럼 비교함\n    - 따라서 클래스 인스턴스가 값으로 비교되는지, 참조로 비교되는지 더는 불확실하지 않게됨\n    \n  - 사용하기 쉽고 다루기 편함"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c89120f3-1df3-474c-b165-977ed55efb6c"}}},{"cell_type":"code","source":["#레코드를 표현할 case class 정의(Flight 데이터 타입의 Dataset)\ncase class Flight(DEST_COUNTRY_NAME:String, ORIGIN_COUNTRY_NAME: String, count: BigInt)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da9ce824-d050-45c3-8f6a-cb4fcb2122f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#데이터를 읽어야 DataFrame이 반환\nval flightsDF = spark.read.parquet('/FileStore/tables/2010_summary.parquet/')\n\n#as 메서드로 Flight 데이터 타입으로 변환\nval flights = flightsDF.as[Flight]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb6fc12d-e19e-4be7-af7a-69dd6e6034af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter11","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3817252953291889}},"nbformat":4,"nbformat_minor":0}
