{"cells":[{"cell_type":"markdown","source":["# 데이터소스\n- 스파크 핵심 데이터소스 \n  - CSV\n  - JSON\n  - 파케이\n  - ORC\n  - JDBC/ODBC연결\n  - 일반 텍스트 파일\n\n- 핵심 서드파티 데이터소스\n  - 카산드라\n  - HBase\n  - 몽고디비\n  - AWS Redshift\n  - XML"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56ae41fb-0f65-49ed-88a9-1a4d589b8d33"}}},{"cell_type":"code","source":["path='/FileStore/tables/all/*.csv'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e15894f8-32d9-41b3-abcf-b30cc8fd63b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 데이터소스 API의 구조"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34a934a5-8b8f-47e8-9cdb-2d4d1df1556b"}}},{"cell_type":"markdown","source":["### 읽기 API 구조\n- 스파크에서 데이터를 읽을 땐 기본적으로 DataFrameReader를 사용함\n- DataFrameReader는 SparkSession의 read 속성으로 접근\n- DataFrameReader를 얻고 나서는 다음과 같은 값을 지정해야함\n  - format: 포맷 지정(default: 파케이)\n  - option: 데이터 읽는 방법 지정 ex)키-값 쌍dlaus option('key', 'value')\n  - schema: 데이터 소스에서 스키마를 제공하거나 스키마 추론 기능을 사용하려고 할 때 지정"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2e91570-8c90-4874-b9d2-d13f8ce7b01b"}}},{"cell_type":"code","source":["spark.read"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9795a76-10cb-4d4f-864c-8e9b192102d9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: &lt;pyspark.sql.readwriter.DataFrameReader at 0x7f6b4e3e9f40&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &lt;pyspark.sql.readwriter.DataFrameReader at 0x7f6b4e3e9f40&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format('csv')\\\n.option('mode', 'FAILFAST')\\\n.option('inferSchema', 'true')\\\n.option('path', path)\\\n.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9513497d-e45d-41ad-8a5d-dfb1c2b603de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n- 위 예시에서는 읽기 모드를 'failfast'로 지정함\n- <strong>읽기 모드: 스파크가 형식에 맞지 않는 데이터를 만났을 때의 동작 방식을 지정하는 옵션</strong>\n  - permissive(default): 오류 레코드의 모든 필드를 null로 설정하고 모든 오류 레코드를 _corrupt_record라는 문자열 컬럼에 기록\n  - dropMalformed: 형식에 맞지 않는 레코드가 포함된 로우 제거\n  - failFast: 형식에 맞지 않는 레코드를 만나면 즉시 종료"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d484434-4fb3-4f4f-b1a1-892fb18b1540"}}},{"cell_type":"markdown","source":["### 쓰기 API 구조\n- 데이터 읽기와 매우 유사한데 DataFrameReader 대신 DataFrameWriter를 사용\n- 데이터소스에 항상 데이터를 기록해야하므로 DataFrame의 write속성을 사용\n  - DataFrame별로 DataFrameWriter에 접근\n- DataFrameWriter를 얻고나서는 다음과 같은 값을 지정해야함\n  - format\n  - option\n  - 파일 기반의 데이터소스만 해당\n    - partitionBy\n    - bucketBy\n    - sortBy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1289f7f-84b5-4e49-9b43-7fdcee133e74"}}},{"cell_type":"code","source":["df.write.format('csv')\\\n.option('mode', 'OVERWRITE')\\\n.option('dateFormat', 'yyyy-MM-dd')\\\n.option('path', '/FileStore/tables/temp/temp.csv')\\\n.save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9178a46c-d3aa-42b0-8926-8ef38bc3d717"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["- 위 예시에서는 저장 모드를 'overwrite'로 지정함\n- 저장 모드: 스파크가 지정된 위치에서 동일한 파일이 발견됐을 때의 동작 방식 지정\n  - append: 해당 경로에 이미 존재하는 파일 목록에 결과 파일 추가\n  - overwrite: 이미 존재하는 모든 데이터를 완전히 덮어씀\n  - errorIfExists(default): 오류를 발생시키면서 쓰기 작업이 실패\n  - ignore: 아무런 처리 X"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c08024d9-6ed4-47f9-b3d5-04146611aad2"}}},{"cell_type":"markdown","source":["## CSV 파일\n- CSV(comma-separated values)는 콤마(,)로 구분된 값을 의미\n- 각 줄이 단일 레코드\n- 각 필드를 콤마로 구분하는 일반적인 텍스트 파일 포맷\n- 해당 포맷이 다루기 까다로운 이유\n  - 운영 환경에서는 어떤 내용, 어떤 구조로 되어 있는지 등 다양한 전제를 만들어낼 수 없기 때문\n  - 그래서 <strong>CSV reader는 많은 수의 옵션</strong>을 제공\n  - [링크](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html?highlight=csv#pyspark.sql.DataFrameReader.csv)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"893f2efe-0ebe-4102-a0a7-8233653b1127"}}},{"cell_type":"markdown","source":["### CSV 파일 읽기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d117336c-609f-4dc5-b1b7-de66dfe97ce2"}}},{"cell_type":"code","source":["spark.read.format('csv')\\\n.option('header', 'true')\\\n.option('mode','FAILFAST')\\\n.load(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17802ef6-e2cb-4bfb-a004-953fcb6c0dcb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: string, InvoiceDate: string, UnitPrice: string, CustomerID: string, Country: string]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: string, InvoiceDate: string, UnitPrice: string, CustomerID: string, Country: string]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### CSV 파일 쓰기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57fccf0d-d54f-40ac-8ea7-b6b1936443cc"}}},{"cell_type":"code","source":["df.write.format('csv').mode('overwrite').option('sep','\\t').save('/FileStore/tables/temp/tsv_file.tsv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c006920c-052d-48a7-be2b-b00cf8bee54a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-----\n- 위처럼 csv파일을 읽어 들여 tsv파일로 내보는 처리도 간단"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c10dc2db-a5a0-4476-94f6-53fa9a14403a"}}},{"cell_type":"markdown","source":["## JSON 파일\n- 자바스크립트 객체 표기법(JavaScript Object Notation)\n- 스파크에서는 JSON파일을 사용할 때 줄로 구분된 JSON을 기본적으로 사용\n  - 큰 JSON 객체나 배열을 하나씩 가지고 있는 파일을 다루는 것과 대조적인 부분\n  - multiLine옵션으로 바꿀 순 있음\n  - 근데 줄로 구분된 방식이 더 안정적이라서 디폴트임\n    - 구조화 되어 있음\n    - 최소한의 기본 데이터 타입이 존재함\n    \n- JSON은 객체이므로 CSV보다 옵션 수가 적음\n  - [링크](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html?highlight=json#pyspark.sql.DataFrameReader.json)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e98673c-eea2-4818-9ce7-1f083de7313f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter9","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1421158351524074}},"nbformat":4,"nbformat_minor":0}
