{"cells":[{"cell_type":"markdown","source":["# 분산형 공유 변수\n- 스파크의 저수준 API에는 RDD 인터페이스 외에도 두 번재 유형인 <strong>분산형 공유 변수</strong>가 있다.\n- 분산형 공유 변수 종류\n  - <strong>브로드캐스트 변수</strong>\n  - <strong>어큐뮬레이터</strong>\n- 어큐뮬레이터를 사용하면 <strong>모든 태스크의 데이터를 공유 결과에 추가</strong>할 수 있음\n- 브로드캐스트 변수를 사용하면 <strong>모든 워커 노드에 큰 값을 저장</strong>하므로 재전송 없이 많은 액션에서 재사용 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e029caf5-d8a7-49de-ae3a-4135f1c8c746"}}},{"cell_type":"markdown","source":["## 브로드캐스트 변수\n- 원래는 태스크에서 드라이버 노드의 변수를 사용할 때 클로저 함수 내부에서 단순하게 참조하는 방법을 사용한다. \n  - 하지만 룩업 테이블이나 머신러닝 모델 같은 <strong>큰 변수</strong>를 사용하는 경우 비효율적\n  - 왜냐면 해당 변수를 사용할 때 워커 노드에서 <strong>태스크당 한 번씩 역직렬화</strong>가 일어나기 때문\n  - 그리고 여러 스파크 액션과 잡에서 동일한 변수를 사용하면 <strong>잡을 실행할 때마다 워커로 큰 변수를 재전송</strong>해야함\n\n- 브로드캐스트 변수는 <strong>변하지 않는 값</strong>을 클러스터에서 효율적으로 <strong>공유</strong>하는 방법 제공\n  - 모든 태스크마다 직렬화하지 않고 클러스터의 <strong>모든 머신에 캐시하는 불변성 공유 변수임</strong>\n  - 액션을 실행할 때 클러스터의 모든 노드에 <strong>지연 처리 방식으로 복제됨</strong>\n  \n  <img src=\"https://mallikarjuna_g.gitbooks.io/spark/content/images/sparkcontext-broadcast-executors.png\" width=70% />\n  \n  \n[참고](https://mallikarjuna_g.gitbooks.io/spark/content/spark-broadcast.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00b550e6-a79e-4bd6-9dcc-76ca7456ffe4"}}},{"cell_type":"code","source":["my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b25c09e-1106-4581-918f-e654dcf05ff7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["words= spark.sparkContext.parallelize(my_collection,2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0486bcfa-a37f-4886-986f-21de7c96c5aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#아래 구조체를 스파크에 브로드캐스트할 수 있음\nsupplementalData = {\"Spark\": 1000, \"Definitive\":200, \"Big\": -300, \"simple\":100}\nsuppBroadcast = spark.sparkContext.broadcast(supplementalData)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e118ddb9-3925-41fd-957a-102c12d204ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#value 메서드로 브로드캐스트된 값 참조\nsuppBroadcast.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a7a0725-b8fe-4ef8-9819-8c5a7bd4be5a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#브로드캐스트된 데이터를 사용해 RDD를 변환할 수 있음\nwords.map(lambda word: (word, suppBroadcast.value.get(word,0)))\\\n.sortBy(lambda wordPair: wordPair[1]).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fd7b686-043e-4360-9e6c-134c6447478e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-------\n- 브로드캐스트 변수에 큰 크기의 데이터를 사용하는 경우라면 전체 태스크에서 데이터를 직렬화하는 데 발생하는 부하가 매우 커질 수 있음\n- RDD말고 UDF나 Dataset에서도 사용할 수 있고, 동일한 효과를 얻을 수 있음"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2649e5e-4eb0-4cbf-a724-0f24916969af"}}},{"cell_type":"markdown","source":["## 어큐뮬레이터\n- 트랜스포메이션 내부의 <strong>다양한 값을 갱신</strong>하는 데 사용\n\n- 어큐뮬레이터는 스파크 클러스터에서 <strong>로우 단위로 안전하게 값을 갱신</strong>할 수 있는 <strong>변경 가능한 변수</strong>를 제공함\n\n- <strong>병렬</strong> 처리 과정에서 효율적으로 사용할 수 있음\n  - 카운터나 합계를 구하는 용도로 사용 가능\n  \n- 어큐뮬레이터 값은 <strong>액션</strong>을 처리하는 과정에서만 갱신\n\n- <strong>각 태스크</strong>에서 어큐뮬레이터를 <strong>한 번만 갱신</strong>하도록 제어\n  - 재시작한 태스크는 갱신 못함\n\n- 이름이 지정된 어큐뮬레이터만 결과가 스파크 UI에 표시됨\n\n[참고](https://mallikarjuna_g.gitbooks.io/spark/content/spark-accumulators.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5a7355f-4d86-4b81-9b1d-f0f2f7b15243"}}},{"cell_type":"code","source":["path='/FileStore/tables/2010-summary.parquet'\nflights = spark.read.parquet(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0798dfaa-dc6e-4407-8371-205259d3695c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#출발지나 도착지가 중국인 항공편의 수를 구하는 어큐뮬레이터 생성\naccChina = spark.sparkContext.accumulator(0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ded55168-8151-4163-9f73-7d39d471e092"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#'China'로 이름 지정\nspark.sparkContext.register(accChina, 'China')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b2113e5-5b34-4dea-94ce-0f050a881408"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["accChina.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6c6f525-c663-4c58-ab69-3011ca692e8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def accChinaFunc(flight_row):\n  destination = flight_row['DEST_COUNTRY_NAME']\n  origin = flight_row['ORIGIN_COUNTRY_NAME']\n  \n  if destination == 'China' or origin == 'China':\n    accChina.add(flight_row['count'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97e56b10-df93-458f-93b9-944d0c5836e3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#foreach 메서드(액션)를 사용해 매 로우마다 위 함수 적용하기\nflights.foreach(lambda flight_row: accChinaFunc(flight_row))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32956360-0257-46cb-bcac-11956bf30109"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["accChina.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"427e6e6e-9364-442f-b794-4fd82c3b9094"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 사용자 정의 어큐뮬레이터\n- 직접 어큐뮬레이터를 정의하고자 한다면, AccumulatorV2 클래스(스칼라) 또는 AccumulatorParam(파이썬)을 상속받아야함\n-"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"353bffd6-f856-4077-9cc6-4a7233613b5b"}}},{"cell_type":"code","source":["from pyspark.accumulators import AccumulatorParam\nclass EvenAccumulator(AccumulatorParam):\n  def __init__(self, cur_val):\n    self.cur_val = 0\n  \n  def add(self, val):\n    if val%2 ==0:\n      self.cur_val += val"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b04dacd-4928-42ff-8714-0de682970d2a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["evenAcc = spark.sparkContext.accumulator([1.0,2.0,3.0,4.0], EvenAccumulator())\nevenAcc.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd43dba6-d49f-41fc-86d4-3b2220f748ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def g(x):\n  global evenAcc\n  evenAcc += [x]*4\n\nrdd = sc.parallelize([1,2,3,4])\nrdd.foreach(g)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8546d8a6-f8c4-40ea-849f-db6ef4108ebd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["evenAcc.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3daef784-b821-4188-bae7-3404925d4f2a"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter14","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2327296008820674}},"nbformat":4,"nbformat_minor":0}
